# ðŸ¤– Ollama Integration Guide

Diese umfassende Anleitung erklÃ¤rt, wie Ollama in die CrudAiApp integriert wird und wie du es selbst einrichten kannst.

## ðŸ“‹ Was ist Ollama?

Ollama ist ein Tool, um groÃŸe Sprachmodelle (LLMs) lokal auf deinem Computer auszufÃ¼hren. Es ermÃ¶glicht:
- **Offline AI-Chats** ohne Internet
- **Datenschutz** - alle Daten bleiben auf deinem Computer
- **Schnelle Antworten** ohne externe API-Kosten
- **Verschiedene Modelle** wie Llama, Mistral, CodeLlama

## ðŸ› ï¸ Ollama Installation

### macOS Installation
```bash
# 1. Ollama herunterladen und installieren
curl -fsSL https://ollama.com/install.sh | sh

# 2. Ollama starten (lÃ¤uft im Hintergrund)
ollama serve

# 3. Erstes Modell herunterladen (z.B. Llama 3.2:3b)
ollama pull llama3.2:3b
```

### Alternative: Manuelle Installation
1. Gehe zu [ollama.com](https://ollama.com)
2. Lade die App fÃ¼r dein Betriebssystem herunter
3. Installiere und starte Ollama
4. Ã–ffne Terminal und fÃ¼hre `ollama pull llama3.2:3b` aus

## ðŸŽ¯ VerfÃ¼gbare Modelle

### Empfohlene Modelle fÃ¼r Chat:
```bash
# Llama 3.2 (3B) - Schnell, gut fÃ¼r Chat
ollama pull llama3.2:3b

# Llama 3.2 (1B) - Sehr schnell, weniger genau  
ollama pull llama3.2:1b

# Mistral (7B) - GrÃ¶ÃŸer, sehr gut fÃ¼r komplexe Fragen
ollama pull mistral:7b

# CodeLlama - Speziell fÃ¼r Programmierung
ollama pull codellama:7b
```

### Modell-Vergleich:
| Modell | GrÃ¶ÃŸe | RAM Bedarf | Geschwindigkeit | QualitÃ¤t |
|--------|-------|------------|----------------|----------|
| llama3.2:1b | 1.3GB | 2GB | âš¡âš¡âš¡ | â­â­â­ |
| llama3.2:3b | 2GB | 4GB | âš¡âš¡ | â­â­â­â­ |
| mistral:7b | 4.1GB | 8GB | âš¡ | â­â­â­â­â­ |

## ðŸ”§ Backend Integration

### 1. AI Service Setup (ai_service.py)

```python
import httpx
import asyncio
from typing import Dict, List, Optional
import logging

# =============================================================================
# OLLAMA AI SERVICE - Lokale KI-Integration
# =============================================================================

class OllamaService:
    """
    Service fÃ¼r die Kommunikation mit dem lokalen Ollama Server
    Verwaltet Konversationen und generiert AI-Antworten
    """
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3.2:3b"):
        self.base_url = base_url
        self.model = model
        self.conversations: Dict[int, List[Dict]] = {}
        
    async def is_available(self) -> bool:
        """
        PrÃ¼ft ob Ollama Server erreichbar ist
        """
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except Exception as e:
            logging.error(f"Ollama nicht verfÃ¼gbar: {e}")
            return False
    
    async def get_available_models(self) -> List[str]:
        """
        Ruft alle verfÃ¼gbaren Modelle vom Ollama Server ab
        """
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [model["name"] for model in data.get("models", [])]
        except Exception as e:
            logging.error(f"Fehler beim Abrufen der Modelle: {e}")
        return []
    
    def _prepare_conversation_context(self, chat_id: int, new_message: str) -> List[Dict]:
        """
        Bereitet den Konversationskontext vor
        FÃ¼gt neue Nachricht zur Chat-Historie hinzu
        """
        if chat_id not in self.conversations:
            self.conversations[chat_id] = []
        
        # Neue Benutzernachricht hinzufÃ¼gen
        self.conversations[chat_id].append({
            "role": "user",
            "content": new_message
        })
        
        # System-Prompt fÃ¼r bessere deutsche Antworten
        messages = [
            {
                "role": "system", 
                "content": "Du bist ein hilfsreicher AI-Assistent. Antworte immer auf Deutsch und sei freundlich und hilfsreich."
            }
        ]
        
        # Bisherige Konversation hinzufÃ¼gen (max. 10 Nachrichten fÃ¼r Performance)
        recent_messages = self.conversations[chat_id][-10:]
        messages.extend(recent_messages)
        
        return messages
    
    async def generate_response(self, chat_id: int, user_message: str) -> str:
        """
        Generiert AI-Antwort fÃ¼r eine Benutzernachricht
        
        Args:
            chat_id: ID des Chats
            user_message: Nachricht vom Benutzer
            
        Returns:
            str: AI-Antwort
        """
        try:
            # Konversationskontext vorbereiten
            messages = self._prepare_conversation_context(chat_id, user_message)
            
            # Anfrage an Ollama senden
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,  # KreativitÃ¤t
                            "top_p": 0.9,       # Fokussierung
                            "top_k": 40         # Wortauswahl
                        }
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    ai_response = data.get("message", {}).get("content", "Keine Antwort erhalten.")
                    
                    # AI-Antwort zur Konversation hinzufÃ¼gen
                    self.conversations[chat_id].append({
                        "role": "assistant",
                        "content": ai_response
                    })
                    
                    return ai_response
                else:
                    return f"Fehler beim Generieren der Antwort (Status: {response.status_code})"
                    
        except asyncio.TimeoutError:
            return "â±ï¸ Die AI-Antwort hat zu lange gedauert. Bitte versuche es erneut."
        except Exception as e:
            logging.error(f"Fehler bei AI-Generierung: {e}")
            return f"âŒ Fehler bei der AI-Antwort: {str(e)}"
    
    async def reset_conversation(self, chat_id: int):
        """
        Setzt die Konversationshistorie fÃ¼r einen Chat zurÃ¼ck
        """
        if chat_id in self.conversations:
            del self.conversations[chat_id]
    
    async def get_conversation_length(self, chat_id: int) -> int:
        """
        Gibt die Anzahl der Nachrichten in einer Konversation zurÃ¼ck
        """
        return len(self.conversations.get(chat_id, []))

# Globale Service-Instanz
ollama_service = OllamaService()
```

### 2. Route Integration (routes.py)

```python
from fastapi import HTTPException
from .ai_service import ollama_service

# AI-Response Endpoint
@app.post("/chats/{chat_id}/ai-response")
async def generate_ai_response(chat_id: int, request: dict):
    """
    Generiert eine AI-Antwort fÃ¼r eine Benutzernachricht
    
    1. PrÃ¼ft ob Ollama verfÃ¼gbar ist
    2. Generiert AI-Antwort
    3. Speichert AI-Antwort in Datenbank
    4. Gibt AI-Nachricht zurÃ¼ck
    """
    try:
        # Chat existiert prÃ¼fen
        chat = db.query(Chat).filter(Chat.id == chat_id).first()
        if not chat:
            raise HTTPException(status_code=404, detail="Chat not found")
        
        # Ollama VerfÃ¼gbarkeit prÃ¼fen
        if not await ollama_service.is_available():
            raise HTTPException(status_code=503, detail="AI-Service nicht verfÃ¼gbar. Ist Ollama gestartet?")
        
        user_message = request.get("user_message", "")
        if not user_message:
            raise HTTPException(status_code=400, detail="user_message ist erforderlich")
        
        # AI-Antwort generieren
        ai_response = await ollama_service.generate_response(chat_id, user_message)
        
        # AI-Antwort in Datenbank speichern
        ai_message = Message(
            content=ai_response,
            chat_id=chat_id,
            is_user=False
        )
        db.add(ai_message)
        db.commit()
        db.refresh(ai_message)
        
        return {
            "id": ai_message.id,
            "content": ai_message.content,
            "chat_id": ai_message.chat_id,
            "is_user": ai_message.is_user,
            "created_at": ai_message.created_at
        }
        
    except Exception as e:
        logging.error(f"Fehler bei AI-Response: {e}")
        raise HTTPException(status_code=500, detail=f"Interner Server-Fehler: {str(e)}")
```

## ðŸŽ¯ Frontend Integration

### API Call fÃ¼r AI-Response (api.ts)
```typescript
// =============================================================================
// AI-INTEGRATION - Ollama Kommunikation
// =============================================================================

/**
 * Generiert eine AI-Antwort fÃ¼r eine Benutzernachricht
 * Kommuniziert mit dem Backend, welches wiederum mit Ollama spricht
 */
export const generateAIResponse = async (chatId: number, userMessage: string): Promise<Message> => {
  try {
    const response = await fetch(`${API_BASE_URL}/chats/${chatId}/ai-response`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        user_message: userMessage,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.detail || 'Failed to generate AI response');
    }

    return response.json();
  } catch (error) {
    console.error('AI Response Error:', error);
    throw error;
  }
};
```

### Chat Integration (chat.ts)
```typescript
// =============================================================================
// CHAT-WORKFLOW MIT AI-INTEGRATION
// =============================================================================

/**
 * Sendet eine Nachricht und generiert automatisch eine AI-Antwort
 */
const handleSendMessage = async () => {
  const messageText = messageInput.value.trim();
  if (!messageText) return;

  try {
    // Input leeren und deaktivieren
    messageInput.value = '';
    messageInput.disabled = true;
    sendButton.disabled = true;

    // 1. Benutzernachricht senden und anzeigen
    const userMessage = await sendMessage(currentChatId, messageText);
    appendMessage({
      ...userMessage,
      content: messageText,
      is_user: true,
      created_at: new Date().toISOString()
    });

    // 2. Typing-Indikator fÃ¼r AI anzeigen
    showTypingIndicator();

    // 3. AI-Antwort generieren
    const aiMessage = await generateAIResponse(currentChatId, messageText);
    
    // 4. Typing-Indikator entfernen und AI-Antwort anzeigen
    hideTypingIndicator();
    appendMessage(aiMessage);

    // 5. Scroll zu neuester Nachricht
    scrollToBottom();

  } catch (error) {
    console.error('Error in message workflow:', error);
    
    // Fehlerbehandlung
    hideTypingIndicator();
    appendMessage({
      id: Date.now(),
      content: `âŒ Fehler: ${error.message}`,
      chat_id: currentChatId,
      is_user: false,
      created_at: new Date().toISOString()
    });
  } finally {
    // Input wieder aktivieren
    messageInput.disabled = false;
    sendButton.disabled = false;
    messageInput.focus();
  }
};

/**
 * Zeigt einen Typing-Indikator fÃ¼r die AI an
 */
const showTypingIndicator = () => {
  const typingDiv = document.createElement('div');
  typingDiv.id = 'typing-indicator';
  typingDiv.className = 'message message-incoming typing-indicator';
  typingDiv.innerHTML = `
    <div class="message-content">
      <div class="typing-dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  `;
  messagesContainer.appendChild(typingDiv);
  scrollToBottom();
};

/**
 * Entfernt den Typing-Indikator
 */
const hideTypingIndicator = () => {
  const typingIndicator = document.getElementById('typing-indicator');
  if (typingIndicator) {
    typingIndicator.remove();
  }
};
```

## ðŸ” Troubleshooting

### HÃ¤ufige Probleme und LÃ¶sungen:

#### 1. "Ollama nicht verfÃ¼gbar" Fehler
```bash
# PrÃ¼fen ob Ollama lÃ¤uft
ps aux | grep ollama

# Ollama starten
ollama serve

# Oder als Service starten (macOS)
brew services start ollama
```

#### 2. "Modell nicht gefunden"
```bash
# VerfÃ¼gbare Modelle anzeigen
ollama list

# Modell herunterladen
ollama pull llama3.2:3b

# Modell testen
ollama run llama3.2:3b "Hallo, wie geht es dir?"
```

#### 3. Langsame Antworten
```bash
# Kleineres Modell verwenden
ollama pull llama3.2:1b

# In ai_service.py das Modell Ã¤ndern:
# self.model = "llama3.2:1b"
```

#### 4. Speicherprobleme
```bash
# RAM-Verbrauch prÃ¼fen
ollama ps

# Nicht verwendete Modelle entfernen
ollama rm mistral:7b
```

## âš™ï¸ Konfiguration & Optimierung

### Backend-Konfiguration anpassen:
```python
# In ai_service.py verschiedene Einstellungen testen:

# FÃ¼r kreativere Antworten:
"options": {
    "temperature": 0.9,  # HÃ¶her = kreativer
    "top_p": 0.8,
    "top_k": 60
}

# FÃ¼r prÃ¤zisere Antworten:
"options": {
    "temperature": 0.3,  # Niedriger = prÃ¤ziser  
    "top_p": 0.95,
    "top_k": 20
}
```

### Performance-Optimierung:
```python
# Konversationshistorie begrenzen (ai_service.py)
recent_messages = self.conversations[chat_id][-5:]  # Nur letzte 5 Nachrichten

# Timeout anpassen
async with httpx.AsyncClient(timeout=60.0) as client:  # KÃ¼rzer fÃ¼r schnellere Fehlerbehandlung
```

## ðŸš€ Erweiterte Features

### 1. Modell-Wechsel zur Laufzeit:
```python
@app.post("/ai/change-model")
async def change_model(model_name: str):
    """Ã„ndert das verwendete AI-Modell"""
    available_models = await ollama_service.get_available_models()
    if model_name not in available_models:
        raise HTTPException(status_code=400, detail="Modell nicht verfÃ¼gbar")
    
    ollama_service.model = model_name
    return {"message": f"Modell geÃ¤ndert zu {model_name}"}
```

### 2. System-Prompt anpassen:
```python
# Verschiedene PersÃ¶nlichkeiten definieren
SYSTEM_PROMPTS = {
    "helpful": "Du bist ein hilfsreicher AI-Assistent.",
    "creative": "Du bist ein kreativer AI-Assistent, der gerne Geschichten erzÃ¤hlt.",
    "technical": "Du bist ein technischer AI-Assistent mit Fokus auf Programmierung.",
}
```

### 3. Streaming-Antworten (Echtzeit):
```python
# FÃ¼r Live-Antworten wÃ¤hrend der Generierung
"stream": True  # In der Ollama-Anfrage
```

## ðŸ“Š Monitoring & Logging

### Performance Ã¼berwachen:
```python
import time
import logging

class OllamaService:
    async def generate_response(self, chat_id: int, user_message: str) -> str:
        start_time = time.time()
        
        try:
            # ... AI-Generierung ...
            response_time = time.time() - start_time
            logging.info(f"AI-Response generiert in {response_time:.2f}s fÃ¼r Chat {chat_id}")
            
        except Exception as e:
            logging.error(f"AI-Fehler nach {time.time() - start_time:.2f}s: {e}")
```

## ðŸŽ‰ Fertig!

Mit dieser Anleitung hast du:
âœ… Ollama installiert und konfiguriert  
âœ… Backend AI-Service implementiert  
âœ… Frontend AI-Integration erstellt  
âœ… Error-Handling und Monitoring eingerichtet  
âœ… Performance-Optimierungen angewendet  

Deine CrudAiApp kann jetzt offline mit lokalen AI-Modellen chatten! ðŸš€
