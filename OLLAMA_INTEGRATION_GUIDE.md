# ü§ñ Ollama Integration Guide

Diese umfassende Anleitung erkl√§rt, wie Ollama in die CrudAiApp integriert wird und wie du es selbst einrichten kannst.

## üìã Was ist Ollama?

Ollama ist ein Tool, um gro√üe Sprachmodelle (LLMs) lokal auf deinem Computer auszuf√ºhren. Es erm√∂glicht:
- **Offline AI-Chats** ohne Internet
- **Datenschutz** - alle Daten bleiben auf deinem Computer
- **Schnelle Antworten** ohne externe API-Kosten
- **Verschiedene Modelle** wie Llama, Mistral, CodeLlama

## üõ†Ô∏è Ollama Installation

### macOS Installation
```bash
# 1. Ollama herunterladen und installieren
curl -fsSL https://ollama.com/install.sh | sh

# 2. Ollama starten (l√§uft im Hintergrund)
ollama serve

# 3. Erstes Modell herunterladen (z.B. Llama 3.2:3b)
ollama pull llama3.2:3b
```

### Alternative: Manuelle Installation
1. Gehe zu [ollama.com](https://ollama.com)
2. Lade die App f√ºr dein Betriebssystem herunter
3. Installiere und starte Ollama
4. √ñffne Terminal und f√ºhre `ollama pull llama3.2:3b` aus

## üéØ Verf√ºgbare Modelle

### Empfohlene Modelle f√ºr Chat:
```bash
# Llama 3.2 (3B) - Schnell, gut f√ºr Chat
ollama pull llama3.2:3b

# Llama 3.2 (1B) - Sehr schnell, weniger genau  
ollama pull llama3.2:1b

# Mistral (7B) - Gr√∂√üer, sehr gut f√ºr komplexe Fragen
ollama pull mistral:7b

# CodeLlama - Speziell f√ºr Programmierung
ollama pull codellama:7b
```

### Modell-Vergleich:
| Modell | Gr√∂√üe | RAM Bedarf | Geschwindigkeit | Qualit√§t |
|--------|-------|------------|----------------|----------|
| llama3.2:1b | 1.3GB | 2GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| llama3.2:3b | 2GB | 4GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| mistral:7b | 4.1GB | 8GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## üîß Backend Integration

### 1. AI Service Setup (ai_service.py)

```python
import httpx
import asyncio
from typing import Dict, List, Optional
import logging

# =============================================================================
# OLLAMA AI SERVICE - Lokale KI-Integration
# =============================================================================

class OllamaService:
    """
    Service f√ºr die Kommunikation mit dem lokalen Ollama Server
    Verwaltet Konversationen und generiert AI-Antworten
    """
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3.2:3b"):
        self.base_url = base_url
        self.model = model
        self.conversations: Dict[int, List[Dict]] = {}
        
    async def is_available(self) -> bool:
        """
        Pr√ºft ob Ollama Server erreichbar ist
        """
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except Exception as e:
            logging.error(f"Ollama nicht verf√ºgbar: {e}")
            return False
    
    async def get_available_models(self) -> List[str]:
        """
        Ruft alle verf√ºgbaren Modelle vom Ollama Server ab
        """
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [model["name"] for model in data.get("models", [])]
        except Exception as e:
            logging.error(f"Fehler beim Abrufen der Modelle: {e}")
        return []
    
    def _prepare_conversation_context(self, chat_id: int, new_message: str) -> List[Dict]:
        """
        Bereitet den Konversationskontext vor
        F√ºgt neue Nachricht zur Chat-Historie hinzu
        """
        if chat_id not in self.conversations:
            self.conversations[chat_id] = []
        
        # Neue Benutzernachricht hinzuf√ºgen
        self.conversations[chat_id].append({
            "role": "user",
            "content": new_message
        })
        
        # System-Prompt f√ºr bessere deutsche Antworten
        messages = [
            {
                "role": "system", 
                "content": "Du bist ein hilfsreicher AI-Assistent. Antworte immer auf Deutsch und sei freundlich und hilfsreich."
            }
        ]
        
        # Bisherige Konversation hinzuf√ºgen (max. 10 Nachrichten f√ºr Performance)
        recent_messages = self.conversations[chat_id][-10:]
        messages.extend(recent_messages)
        
        return messages
    
    async def generate_response(self, chat_id: int, user_message: str) -> str:
        """
        Generiert AI-Antwort f√ºr eine Benutzernachricht
        
        Args:
            chat_id: ID des Chats
            user_message: Nachricht vom Benutzer
            
        Returns:
            str: AI-Antwort
        """
        try:
            # Konversationskontext vorbereiten
            messages = self._prepare_conversation_context(chat_id, user_message)
            
            # Anfrage an Ollama senden
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,  # Kreativit√§t
                            "top_p": 0.9,       # Fokussierung
                            "top_k": 40         # Wortauswahl
                        }
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    ai_response = data.get("message", {}).get("content", "Keine Antwort erhalten.")
                    
                    # AI-Antwort zur Konversation hinzuf√ºgen
                    self.conversations[chat_id].append({
                        "role": "assistant",
                        "content": ai_response
                    })
                    
                    return ai_response
                else:
                    return f"Fehler beim Generieren der Antwort (Status: {response.status_code})"
                    
        except asyncio.TimeoutError:
            return "‚è±Ô∏è Die AI-Antwort hat zu lange gedauert. Bitte versuche es erneut."
        except Exception as e:
            logging.error(f"Fehler bei AI-Generierung: {e}")
            return f"‚ùå Fehler bei der AI-Antwort: {str(e)}"
    
    async def reset_conversation(self, chat_id: int):
        """
        Setzt die Konversationshistorie f√ºr einen Chat zur√ºck
        """
        if chat_id in self.conversations:
            del self.conversations[chat_id]
    
    async def get_conversation_length(self, chat_id: int) -> int:
        """
        Gibt die Anzahl der Nachrichten in einer Konversation zur√ºck
        """
        return len(self.conversations.get(chat_id, []))

# Globale Service-Instanz
ollama_service = OllamaService()
```

### 2. Route Integration (routes.py)

```python
from fastapi import HTTPException
from .ai_service import ollama_service

# AI-Response Endpoint
@app.post("/chats/{chat_id}/ai-response")
async def generate_ai_response(chat_id: int, request: dict):
    """
    Generiert eine AI-Antwort f√ºr eine Benutzernachricht
    
    1. Pr√ºft ob Ollama verf√ºgbar ist
    2. Generiert AI-Antwort
    3. Speichert AI-Antwort in Datenbank
    4. Gibt AI-Nachricht zur√ºck
    """
    try:
        # Chat existiert pr√ºfen
        chat = db.query(Chat).filter(Chat.id == chat_id).first()
        if not chat:
            raise HTTPException(status_code=404, detail="Chat not found")
        
        # Ollama Verf√ºgbarkeit pr√ºfen
        if not await ollama_service.is_available():
            raise HTTPException(status_code=503, detail="AI-Service nicht verf√ºgbar. Ist Ollama gestartet?")
        
        user_message = request.get("user_message", "")
        if not user_message:
            raise HTTPException(status_code=400, detail="user_message ist erforderlich")
        
        # AI-Antwort generieren
        ai_response = await ollama_service.generate_response(chat_id, user_message)
        
        # AI-Antwort in Datenbank speichern
        ai_message = Message(
            content=ai_response,
            chat_id=chat_id,
            is_user=False
        )
        db.add(ai_message)
        db.commit()
        db.refresh(ai_message)
        
        return {
            "id": ai_message.id,
            "content": ai_message.content,
            "chat_id": ai_message.chat_id,
            "is_user": ai_message.is_user,
            "created_at": ai_message.created_at
        }
        
    except Exception as e:
        logging.error(f"Fehler bei AI-Response: {e}")
        raise HTTPException(status_code=500, detail=f"Interner Server-Fehler: {str(e)}")
```

## üéØ Frontend Integration

### API Call f√ºr AI-Response (api.ts)
```typescript
// =============================================================================
// AI-INTEGRATION - Ollama Kommunikation
// =============================================================================

/**
 * Generiert eine AI-Antwort f√ºr eine Benutzernachricht
 * Kommuniziert mit dem Backend, welches wiederum mit Ollama spricht
 */
export const generateAIResponse = async (chatId: number, userMessage: string): Promise<Message> => {
  try {
    const response = await fetch(`${API_BASE_URL}/chats/${chatId}/ai-response`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        user_message: userMessage,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.detail || 'Failed to generate AI response');
    }

    return response.json();
  } catch (error) {
    console.error('AI Response Error:', error);
    throw error;
  }
};
```

### Chat Integration (chat.ts)
```typescript
// =============================================================================
// CHAT-WORKFLOW MIT AI-INTEGRATION
// =============================================================================

/**
 * Sendet eine Nachricht und generiert automatisch eine AI-Antwort
 */
const handleSendMessage = async () => {
  const messageText = messageInput.value.trim();
  if (!messageText) return;

  try {
    // Input leeren und deaktivieren
    messageInput.value = '';
    messageInput.disabled = true;
    sendButton.disabled = true;

    // 1. Benutzernachricht senden und anzeigen
    const userMessage = await sendMessage(currentChatId, messageText);
    appendMessage({
      ...userMessage,
      content: messageText,
      is_user: true,
      created_at: new Date().toISOString()
    });

    // 2. Typing-Indikator f√ºr AI anzeigen
    showTypingIndicator();

    // 3. AI-Antwort generieren
    const aiMessage = await generateAIResponse(currentChatId, messageText);
    
    // 4. Typing-Indikator entfernen und AI-Antwort anzeigen
    hideTypingIndicator();
    appendMessage(aiMessage);

    // 5. Scroll zu neuester Nachricht
    scrollToBottom();

  } catch (error) {
    console.error('Error in message workflow:', error);
    
    // Fehlerbehandlung
    hideTypingIndicator();
    appendMessage({
      id: Date.now(),
      content: `‚ùå Fehler: ${error.message}`,
      chat_id: currentChatId,
      is_user: false,
      created_at: new Date().toISOString()
    });
  } finally {
    // Input wieder aktivieren
    messageInput.disabled = false;
    sendButton.disabled = false;
    messageInput.focus();
  }
};

/**
 * Zeigt einen Typing-Indikator f√ºr die AI an
 */
const showTypingIndicator = () => {
  const typingDiv = document.createElement('div');
  typingDiv.id = 'typing-indicator';
  typingDiv.className = 'message message-incoming typing-indicator';
  typingDiv.innerHTML = `
    <div class="message-content">
      <div class="typing-dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  `;
  messagesContainer.appendChild(typingDiv);
  scrollToBottom();
};

/**
 * Entfernt den Typing-Indikator
 */
const hideTypingIndicator = () => {
  const typingIndicator = document.getElementById('typing-indicator');
  if (typingIndicator) {
    typingIndicator.remove();
  }
};
```

## üîç Troubleshooting

### H√§ufige Probleme und L√∂sungen:

#### 1. "Ollama nicht verf√ºgbar" Fehler
```bash
# Pr√ºfen ob Ollama l√§uft
ps aux | grep ollama

# Ollama starten
ollama serve

# Oder als Service starten (macOS)
brew services start ollama
```

#### 2. "Modell nicht gefunden"
```bash
# Verf√ºgbare Modelle anzeigen
ollama list

# Modell herunterladen
ollama pull llama3.2:3b

# Modell testen
ollama run llama3.2:3b "Hallo, wie geht es dir?"
```

#### 3. Langsame Antworten
```bash
# Kleineres Modell verwenden
ollama pull llama3.2:1b

# In ai_service.py das Modell √§ndern:
# self.model = "llama3.2:1b"
```

#### 4. Speicherprobleme
```bash
# RAM-Verbrauch pr√ºfen
ollama ps

# Nicht verwendete Modelle entfernen
ollama rm mistral:7b
```

## ‚öôÔ∏è Konfiguration & Optimierung

### Backend-Konfiguration anpassen:
```python
# In ai_service.py verschiedene Einstellungen testen:

# F√ºr kreativere Antworten:
"options": {
    "temperature": 0.9,  # H√∂her = kreativer
    "top_p": 0.8,
    "top_k": 60
}

# F√ºr pr√§zisere Antworten:
"options": {
    "temperature": 0.3,  # Niedriger = pr√§ziser  
    "top_p": 0.95,
    "top_k": 20
}
```

### Performance-Optimierung:
```python
# Konversationshistorie begrenzen (ai_service.py)
recent_messages = self.conversations[chat_id][-5:]  # Nur letzte 5 Nachrichten

# Timeout anpassen
async with httpx.AsyncClient(timeout=60.0) as client:  # K√ºrzer f√ºr schnellere Fehlerbehandlung
```

## üöÄ Erweiterte Features

### 1. Modell-Wechsel zur Laufzeit:
```python
@app.post("/ai/change-model")
async def change_model(model_name: str):
    """√Ñndert das verwendete AI-Modell"""
    available_models = await ollama_service.get_available_models()
    if model_name not in available_models:
        raise HTTPException(status_code=400, detail="Modell nicht verf√ºgbar")
    
    ollama_service.model = model_name
    return {"message": f"Modell ge√§ndert zu {model_name}"}
```

### 2. System-Prompt anpassen:
```python
# Verschiedene Pers√∂nlichkeiten definieren
SYSTEM_PROMPTS = {
    "helpful": "Du bist ein hilfsreicher AI-Assistent.",
    "creative": "Du bist ein kreativer AI-Assistent, der gerne Geschichten erz√§hlt.",
    "technical": "Du bist ein technischer AI-Assistent mit Fokus auf Programmierung.",
}
```

### 3. Streaming-Antworten (Echtzeit):
```python
# F√ºr Live-Antworten w√§hrend der Generierung
"stream": True  # In der Ollama-Anfrage
```

## üìä Monitoring & Logging

### Performance √ºberwachen:
```python
import time
import logging

class OllamaService:
    async def generate_response(self, chat_id: int, user_message: str) -> str:
        start_time = time.time()
        
        try:
            # ... AI-Generierung ...
            response_time = time.time() - start_time
            logging.info(f"AI-Response generiert in {response_time:.2f}s f√ºr Chat {chat_id}")
            
        except Exception as e:
            logging.error(f"AI-Fehler nach {time.time() - start_time:.2f}s: {e}")
```

## üéâ Fertig!

Mit dieser Anleitung hast du:
‚úÖ Ollama installiert und konfiguriert  
‚úÖ Backend AI-Service implementiert  
‚úÖ Frontend AI-Integration erstellt  
‚úÖ Error-Handling und Monitoring eingerichtet  
‚úÖ Performance-Optimierungen angewendet  

Deine CrudAiApp kann jetzt offline mit lokalen AI-Modellen chatten! üöÄ
